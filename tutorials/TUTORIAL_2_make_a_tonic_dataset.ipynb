{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d00650-9e90-417b-8989-5f155d29fb51",
   "metadata": {},
   "source": [
    "# Make a [tonic](https://tonic.readthedocs.io/en/latest/) dataset - see [documentation](https://tonic.readthedocs.io/en/latest/tutorials/wrapping_own_data.html)\n",
    "## Example with the synthetic dataset provided by NTUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93039d8-2fbc-496b-9e01-d12bc833d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antoine/homhots/aprovhots/notebook\n"
     ]
    }
   ],
   "source": [
    "%cd ../notebook/\n",
    "from dataset_creation import Synthetic_Dataset\n",
    "from hots.utils import get_dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1daa0d0-b8ef-4d78-8b08-7219ec56064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset\n",
      "loading -> only_sea_session_3.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41480/41480 [43:06<00:00, 16.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 176640/177010 [00:45<00:00, 3900.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -> only_sea_session_2.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79654/79654 [1:55:37<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 339712/339972 [01:57<00:00, 2902.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -> only_sea_session_1.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|███                                                                                                                                                                                                                                 | 990/74514 [00:05<05:24, 226.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file corrupted at row number 941\n",
      "file corrupted at row number 942\n",
      "file corrupted at row number 943\n",
      "file corrupted at row number 944\n",
      "file corrupted at row number 945\n",
      "file corrupted at row number 946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74514/74514 [1:13:10<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 318464/318756 [01:18<00:00, 4054.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -> only_ground_session_1.csv ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                     | 18466/38163 [39:45<57:37,  5.70it/s]"
     ]
    }
   ],
   "source": [
    "# path where you'll go to find your .csv files to make the dataset with\n",
    "path = '../../Data/2021-12-06_simulator_data/'\n",
    "# gives a patch_size to divide spatially the event streams\n",
    "patch_size = (8,8)\n",
    "# gives a max duration for the samples of the dataset to divide temporally the event streams\n",
    "max_duration = 1e3\n",
    "# labels given to the different classes of the dataset\n",
    "labelz = ['sea','gro']\n",
    "# original sensor_size of the DVS (width,height,polarity)\n",
    "sensor_size = (128, 128, 2)\n",
    "# discard samples with less than min_num_events events\n",
    "min_num_events = 1000\n",
    "# split the recordings into train and test sets with train_test_ratio ratio\n",
    "train_test_ratio = .75\n",
    "# gives the indexing of the event stream\n",
    "ordering = 'xytp'\n",
    "\n",
    "trainset = Synthetic_Dataset(save_to=path, train=True, patch_size=patch_size, max_duration=max_duration)\n",
    "testset = Synthetic_Dataset(save_to=path, train=False, patch_size=patch_size, max_duration=max_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9302519-b6fb-4c7d-9de6-93de9dac55cd",
   "metadata": {},
   "source": [
    "**Synthetic_Dataset** goes to find the repository where the dataset with the corresponding *patch_size* and *max_duration* is stored. It is stored in *path/patch_{patch_size}_duration_{max_duration}/* and splitted into *./train/* and *./test/* repositories which are splitted into labelz repositories. \n",
    "\n",
    "If this repository does not exists it calls the function **build_aprovis_dataset** and makes the dataset storage by loading all the .csv files into *path* and treat them with the function **save_as_patches**.\n",
    "\n",
    "The advantage of using the class defined by torch or tonic modules to make a dataset is that we can easily make a loader of the samples and use some transforms already existing in these modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb4376-60f5-475a-8177-e9d6620bdb8c",
   "metadata": {},
   "source": [
    "# Get information about the dataset\n",
    "Some statistics of the dataset can be easily observed with the **get_dataset_info** function. You have to put trainset and testset as input, if you only have one dataset you can put it twice. \n",
    "\n",
    "The default properties to be visualized are:\n",
    "- the *mean_isi* or mean Inter-Spike Interval: the time difference between to consecutive events (polarity is taken into account and then the ISI is computed for events of the same polarity and then averaged). This measure is inversely proportional to the density of events in the event stream. \n",
    "- the *null_isi* or the ratio of synchronous events in the recording: the ratio of events when the ISI is equal to 0 meaning they arrive at the exact same timing. A big *null_isi* value means that there is a lot of synchrnous events, maybe because the event stream is made from frames. \n",
    "- the number of events per sample (indicative when the duration of the samples is the same)\n",
    "- other properties can be asked to be plotted like the *median_isi* or can be developed in **get_properties** in order to visualize different features of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06835db-82c9-43de-b646-eaeaebee38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = get_dataset_info(trainset,testset,distinguish_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fa083-b012-4f53-b896-8e68fabacbb8",
   "metadata": {},
   "source": [
    "Orange histograms are for the \"ground\" label and blue for the \"sea\":\n",
    "- one can notice that ground sessions contain way more events that sea ones (figure on the right)\n",
    "- this fact is also illustrated by the left figure with the Inter-Spike interval\n",
    "- on the middle the ratio of synchronous events are represented. One can see that the values are above .9 and quite close to 1 meaning that there are a lot of events with same timestamps. The fact that the output of the simulator may create frames and then convert them to events may trigger this artifact. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
